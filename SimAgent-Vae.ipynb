{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents:\n",
    "These simulations evaluate several agents exploring the thread the needle enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# from IPython.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 3.10.11 (main, Apr 20 2023, 13:58:42) [Clang 14.0.6 ]\n",
      "torch 2.0.1\n",
      "device = mps\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from state_inference.gridworld_env import CnnWrapper, ThreadTheNeedleEnv\n",
    "from state_inference.utils.training_utils import parse_task_config, parse_model_config\n",
    "from state_inference.utils.pytorch_utils import DEVICE\n",
    "from state_inference.model.agents import ViAgentWithExploration\n",
    "from state_inference.model.vae import MlpEncoder, MlpDecoder, StateVae\n",
    "\n",
    "print(f\"python {sys.version}\")\n",
    "print(f\"torch {torch.__version__}\")\n",
    "print(f\"device = {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_CONFIG_FILE = \"state_inference/env_config.yml\"\n",
    "TASK_NAME = \"thread_the_needle\"\n",
    "TASK_CLASS = ThreadTheNeedleEnv\n",
    "\n",
    "MODEL_CONFIG_FILE = \"state_inference/model/model_config.yml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8d8f013fb746cbb6161740fcb62c74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# create the task\u001b[39;00m\n\u001b[1;32m      7\u001b[0m task \u001b[38;5;241m=\u001b[39m CnnWrapper(TASK_CLASS\u001b[38;5;241m.\u001b[39mcreate_env(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39menv_kwargs))\n\u001b[0;32m----> 9\u001b[0m pi, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_optimal_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m training_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimal_policy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pi\n",
      "File \u001b[0;32m~/Projects/StateInference/state_inference/gridworld_env.py:453\u001b[0m, in \u001b[0;36mGridWorldEnv.get_optimal_policy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    451\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransition_model\u001b[38;5;241m.\u001b[39mstate_action_transitions\n\u001b[1;32m    452\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_model\u001b[38;5;241m.\u001b[39mconstruct_rew_func(t)\n\u001b[0;32m--> 453\u001b[0m sa_values, values \u001b[38;5;241m=\u001b[39m \u001b[43mValueIterationNetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_iteration\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    462\u001b[0m     np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39misclose(v, v\u001b[38;5;241m.\u001b[39mmax()) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m sa_values], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m),\n\u001b[1;32m    463\u001b[0m     values,\n\u001b[1;32m    464\u001b[0m )\n",
      "File \u001b[0;32m~/Projects/StateInference/value_iteration/models/value_iteration_network.py:122\u001b[0m, in \u001b[0;36mValueIterationNetwork.value_iteration\u001b[0;34m(transition_functions, reward_functions, n_rows, n_columns, gamma, iterations, initialization_noise, return_interim_estimates)\u001b[0m\n\u001b[1;32m    119\u001b[0m value_0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(q_0, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a, (r_a, t_a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(reward_functions, transition_functions)):\n\u001b[0;32m--> 122\u001b[0m     q_1[a] \u001b[38;5;241m=\u001b[39m r_a[\u001b[38;5;241m~\u001b[39mtiling] \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m \u001b[43mt_a\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mtiling\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtiling\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m value_1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(q_1, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_interim_estimates:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env_kwargs, training_kwargs = parse_task_config(TASK_NAME, TASK_CONFIG_FILE)\n",
    "\n",
    "training_kwargs[\"n_train_steps\"] = 50000\n",
    "training_kwargs[\"n_epochs\"] = 1\n",
    "\n",
    "# create the task\n",
    "task = CnnWrapper(TASK_CLASS.create_env(**env_kwargs))\n",
    "\n",
    "pi, _ = task.get_optimal_policy()\n",
    "training_kwargs[\"optimal_policy\"] = pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model + Training Parameters\n",
    "\n",
    "model_config = parse_model_config(MODEL_CONFIG_FILE)\n",
    "\n",
    "def make_model():\n",
    "    \n",
    "    vae_model = StateVae.make_from_configs(model_config, env_kwargs)\n",
    "\n",
    "    agent = ViAgentWithExploration(\n",
    "        task,\n",
    "        vae_model,\n",
    "        set_action=set(range(4)),\n",
    "        **model_config[\"state_inference_model\"],\n",
    "    )\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = make_model()\n",
    "print(agent.state_inference_model)\n",
    "total_params = sum(p.numel() for p in agent.state_inference_model.parameters())\n",
    "print(f\"Number of parameters: {total_params}\")\n",
    "agent.learn(30000, estimate_batch=True, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from state_inference.utils.training_utils import get_policy_prob, vae_get_pmf\n",
    "\n",
    "pmf = get_policy_prob(\n",
    "    agent,\n",
    "    vae_get_pmf,\n",
    "    n_states=env_kwargs[\"n_states\"],\n",
    "    map_height=env_kwargs[\"map_height\"],\n",
    "    cnn=True,\n",
    ")\n",
    "pmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(2, 2)\n",
    "h, w = env_kwargs[\"height\"], env_kwargs[\"width\"]\n",
    "\n",
    "axes[0][0].imshow(pmf[:, 0].reshape(h, w))\n",
    "axes[0][1].imshow(pmf[:, 1].reshape(h, w))\n",
    "axes[1][0].imshow(pmf[:, 2].reshape(h, w))\n",
    "axes[1][1].imshow(pmf[:, 3].reshape(h, w))\n",
    "\n",
    "\n",
    "axes[0][0].set_title(\"up\")\n",
    "axes[0][1].set_title(\"down\")\n",
    "axes[1][0].set_title(\"left\")\n",
    "axes[1][1].set_title(\"right\")\n",
    "\n",
    "plt.subplots_adjust(hspace=0.3, wspace=-0.3)\n",
    "\n",
    "plt.suptitle(\"Value Iteration Agent Learned Policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(pi * pmf, axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "room_1_mask = (np.arange(400) < 200) * (np.arange(400) % 20 < 10)\n",
    "room_2_mask = (np.arange(400) >= 200) * (np.arange(400) % 20 < 10)\n",
    "room_3_mask = np.arange(400) % 20 >= 10\n",
    "\n",
    "score_room_1 = np.sum(pi[room_1_mask] * pmf[room_1_mask], axis=1).mean()\n",
    "score_room_2 = np.sum(pi[room_2_mask] * pmf[room_2_mask], axis=1).mean()\n",
    "score_room_3 = np.sum(pi[room_3_mask] * pmf[room_3_mask], axis=1).mean()\n",
    "plt.bar([0, 1, 2], [score_room_1, score_room_2, score_room_3])\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from state_inference.utils.pytorch_utils import make_tensor, convert_8bit_to_float\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "obs = convert_8bit_to_float(\n",
    "    torch.stack(\n",
    "        [\n",
    "            make_tensor(task.observation_model(s))\n",
    "            for s in range(task.transition_model.n_states)\n",
    "            for _ in range(1)\n",
    "        ]\n",
    "    )\n",
    ")[:, None, ...].to(DEVICE)\n",
    "z = agent.state_inference_model.get_state(obs)\n",
    "\n",
    "hash_vector = np.array(\n",
    "    [\n",
    "        agent.state_inference_model.z_dim**ii\n",
    "        for ii in range(agent.state_inference_model.z_layers)\n",
    "    ]\n",
    ")\n",
    "\n",
    "z = z.dot(hash_vector)\n",
    "d = pairwise_distances(z.reshape(-1, 1), metric=lambda x, y: x == y)\n",
    "plt.imshow(1 - d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the overlap of different states\n",
    "# number the states and plot them\n",
    "clusters = {}\n",
    "k = 0\n",
    "for z0 in sorted(z):\n",
    "    if z0 not in clusters.keys():\n",
    "        clusters[z0] = k\n",
    "        k += 1\n",
    "clustered_states = np.array([clusters[z0] for z0 in z])\n",
    "plt.imshow(clustered_states.reshape(-1, 20))\n",
    "task.display_gridworld(plt.gca(), wall_color=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euc = pairwise_distances(\n",
    "    [(x, y) for x in range(20) for y in range(20)],\n",
    "    # metric=lambda x, y: np.sqrt((x[0] - y[0]) ** 2 + (x[1] - y[1]) ** 2),\n",
    "    metric=lambda x, y: np.abs(x[0] - y[0]) + np.abs(x[1] - y[1]),\n",
    ")\n",
    "\n",
    "d_w_wall = np.mean([d[s1][s2] for s1, s2 in task.transition_model.walls])\n",
    "print(f\"Distance between neighboring states sepearted by a wall     {d_w_wall}\")\n",
    "\n",
    "\n",
    "wall_mask = np.zeros((task.n_states, task.n_states))\n",
    "for s0, s1 in task.transition_model.walls:\n",
    "    wall_mask[s0][s1] = 1.0\n",
    "    wall_mask[s1][s0] = 1.0\n",
    "\n",
    "\n",
    "d_wo_wall = d.reshape(-1)[(wall_mask.reshape(-1) == 0) & (euc.reshape(-1) == 1)].mean()\n",
    "print(f\"Distance between neighboring states NOT sepearted by a wall {d_wo_wall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent._estimate_reward_model()\n",
    "\n",
    "rews = np.array([agent.reward_estimator.get_reward(z0) for z0 in z]).reshape(20, 20)\n",
    "plt.imshow(rews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = convert_8bit_to_float(\n",
    "    torch.stack(\n",
    "        [\n",
    "            make_tensor(task.observation_model(s))\n",
    "            for s in range(task.transition_model.n_states)\n",
    "            for _ in range(1)\n",
    "        ]\n",
    "    )\n",
    ")[:, None, ...].to(DEVICE)\n",
    "z = agent.state_inference_model.get_state(obs)\n",
    "\n",
    "hash_vector = np.array(\n",
    "    [\n",
    "        agent.state_inference_model.z_dim**ii\n",
    "        for ii in range(agent.state_inference_model.z_layers)\n",
    "    ]\n",
    ")\n",
    "\n",
    "z = z.dot(hash_vector)\n",
    "\n",
    "rews = np.array([agent.reward_estimator.get_reward(z0) for z0 in z]).reshape(20, 20)\n",
    "plt.imshow(rews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_function(model, task):\n",
    "    obs = convert_8bit_to_float(\n",
    "        torch.stack(\n",
    "            [\n",
    "                make_tensor(task.observation_model(s))\n",
    "                for s in range(task.transition_model.n_states)\n",
    "                for _ in range(1)\n",
    "            ]\n",
    "        )\n",
    "    )[:, None, ...].to(DEVICE)\n",
    "    z = model.state_inference_model.get_state(obs)\n",
    "\n",
    "    hash_vector = np.array(\n",
    "        [\n",
    "            model.state_inference_model.z_dim**ii\n",
    "            for ii in range(agent.state_inference_model.z_layers)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    z = z.dot(hash_vector)\n",
    "\n",
    "    value_function = np.array(\n",
    "        [agent.value_function.get(z0, np.nan) for z0 in z]\n",
    "    ).reshape(20, 20)\n",
    "    return value_function\n",
    "\n",
    "\n",
    "v = get_value_function(agent, task)\n",
    "plt.imshow(v)\n",
    "task.display_gridworld(plt.gca(), wall_color=\"w\", annotate=True)\n",
    "plt.title(\"Learned Value function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(v[5] - np.nanmin(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "\n",
    "## Repeat with iterations\n",
    "n_models = 15\n",
    "# n_models=4\n",
    "\n",
    "\n",
    "room_1_mask = (np.arange(400) < 200) * (np.arange(400) % 20 < 10)\n",
    "room_2_mask = (np.arange(400) >= 200) * (np.arange(400) % 20 < 10)\n",
    "room_3_mask = np.arange(400) % 20 >= 10\n",
    "\n",
    "\n",
    "scores = []\n",
    "value_functions = []\n",
    "\n",
    "\n",
    "for idx in trange(n_models):\n",
    "    agent = make_model()\n",
    "    agent.learn(total_timesteps=training_kwargs[\"n_train_steps\"], progress_bar=False)\n",
    "    #     agent.learn(total_timesteps=500, progress_bar=False)\n",
    "\n",
    "    pmf = get_policy_prob(\n",
    "        agent,\n",
    "        vae_get_pmf,\n",
    "        n_states=env_kwargs[\"n_states\"],\n",
    "        map_height=env_kwargs[\"map_height\"],\n",
    "        cnn=True,\n",
    "    )\n",
    "\n",
    "    score_room_1 = np.sum(pi[room_1_mask] * pmf[room_1_mask], axis=1).mean()\n",
    "    score_room_2 = np.sum(pi[room_2_mask] * pmf[room_2_mask], axis=1).mean()\n",
    "    score_room_3 = np.sum(pi[room_3_mask] * pmf[room_3_mask], axis=1).mean()\n",
    "\n",
    "    v = get_value_function(agent, task)\n",
    "\n",
    "    scores.append(\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"Iteration\": [idx] * 4,\n",
    "                \"Score\": [\n",
    "                    np.sum(pi * pmf, axis=1).mean(),\n",
    "                    score_room_1,\n",
    "                    score_room_2,\n",
    "                    score_room_3,\n",
    "                ],\n",
    "                \"Condition\": [\"Overall\", \"Room 1\", \"Room 2\", \"Room 3\"],\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "    value_functions.append(\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"Iteration\": [idx] * task.n_states,\n",
    "                \"State-Values\": v.reshape(-1),\n",
    "                \"States\": np.arange(task.n_states),\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "scores = pd.concat(scores)\n",
    "value_functions = pd.concat(value_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the average value function (n)\n",
    "\n",
    "# normalize the value function between zero and one within each iteration\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def min_max_scale(grouped_data):\n",
    "    v = grouped_data[\"State-Values\"].values.reshape(-1, 1)\n",
    "    v = MinMaxScaler().fit_transform(grouped_data[\"State-Values\"].values.reshape(-1, 1))\n",
    "    grouped_data[\"State-Values\"] = v\n",
    "    return grouped_data.drop(\"Iteration\", axis=1)\n",
    "\n",
    "\n",
    "normed_vf = value_functions.groupby(\"Iteration\", group_keys=True).apply(min_max_scale)\n",
    "\n",
    "# average and plot\n",
    "plt.imshow(x.groupby(\"States\").mean().values.reshape(20, 20))\n",
    "task.display_gridworld(plt.gca(), wall_color=\"w\", annotate=True)\n",
    "plt.title(\"Learned Value function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 1d Value function through the goal\n",
    "normed_vf[\"Row\"] = normed_vf[\"States\"] // 20\n",
    "normed_vf[\"Column\"] = normed_vf[\"States\"] % 20\n",
    "\n",
    "sns.relplot(\n",
    "    data=normed_vf[normed_vf[\"Row\"] == 4], x=\"Column\", y=\"State-Values\", kind=\"line\"\n",
    ")\n",
    "sns.relplot(\n",
    "    data=normed_vf[(normed_vf[\"Column\"] >= 9) & (normed_vf[\"Column\"] <= 10)],\n",
    "    x=\"Row\",\n",
    "    y=\"State-Values\",\n",
    "    kind=\"line\",\n",
    "    hue=\"Column\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.imshow(\n",
    "    value_functions.groupby(\"States\")[\"State-Values\"]\n",
    "    .apply(np.nanmean)\n",
    "    .values.reshape(20, 20)\n",
    ")\n",
    "value_functions.to_csv('value_functions_vae.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=scores, y=\"Score\", x=\"Condition\", kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[\"Model\"] = \"Value Iteration\"\n",
    "scores.to_csv(\"sims_thread_the_needle_vi_agent_new.csv\")\n",
    "scores2 = pd.read_csv(\"sims_thread_the_needle.csv\")\n",
    "scores2[\"Model\"] = \"PPO\"\n",
    "scores3 = pd.read_csv(\"sims_thread_the_needle_state_inf.csv\")\n",
    "scores3[\"Model\"] = \"Value Iteration + action based decoder\"\n",
    "\n",
    "\n",
    "all_scores = pd.concat([scores, scores2, scores3])\n",
    "sns.catplot(\n",
    "    data=all_scores[all_scores[\"Condition\"] != \"Overall\"],\n",
    "    y=\"Score\",\n",
    "    x=\"Condition\",\n",
    "    kind=\"point\",\n",
    "    hue=\"Model\",\n",
    ")\n",
    "plt.gca().set_ylim([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(\n",
    "    data=all_scores[all_scores[\"Condition\"] == \"Overall\"],\n",
    "    y=\"Score\",\n",
    "    x=\"Model\",\n",
    "    kind=\"bar\",\n",
    ")\n",
    "plt.gca().set_ylim([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
