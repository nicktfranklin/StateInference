state_inference_model:
  batch_size: 16
  n_epochs: 20
  grad_clip: True
  optim_kwargs:
    lr: 0.0003

vae_kwargs:
  z_dim: 12 
  z_layers: 32
  beta: 1.0
  tau: 0.5
  gamma: 0.9997 

mlp_encoder:
  dropout: 0.00

mlp_decoder:
  dropout: 0.00


cnn_encoder:
  in_channels: 1
  channels:
    - 16
    - 32
    - 64
    - 128
    - 256

cnn_decoder:
  channel_out: 1
  channels:
    - 256
    - 128
    - 64
    - 32
    - 16

value_iteration_kwargs:
  gamma: 0.99
  n_iter: 1000
  softmax_gain: 1.0
  epsilon: 0.05
  batch_length: None

q_learning_kwargs:
  alpha: 0.05

